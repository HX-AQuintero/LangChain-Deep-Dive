{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgC5tGKUxJTl"
      },
      "source": [
        "# Deep Dive into LangChain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvZOy02i0ria",
        "outputId": "b9597dc2-3ee1-4647-e045-2b05cb70310d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/LangChain Deep Dive\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/Colab Notebooks/LangChain Deep Dive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBmCpLlc00q1",
        "outputId": "2ee5ad64-eb43-419e-a7df-d2f629f13ed3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangChain_Deep_Dive.ipynb  requirements.txt\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hA6UmdxV03ra",
        "outputId": "0d254a07-cda7-47a7-88f4-a8ba2738f629"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "pip install -r requirements.txt -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbNyBcSS1D5b"
      },
      "outputs": [],
      "source": [
        "#pip show openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OX5jOpHs1P6N"
      },
      "source": [
        "### Python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f5jxVTI1ecs",
        "outputId": "96c910a8-e55a-4dbc-bc65-1ca0bb28f845"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "load_dotenv(find_dotenv(), override=True)\n",
        "\n",
        "# os.environ.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14wY2Qbx5nvM"
      },
      "source": [
        "# Chat Models: GPT-3.5 Turbo and GPT-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qpGpz3tvwSJZ",
        "outputId": "c030c689-5149-453a-816d-2595aab8624f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/62.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/437.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m430.1/437.6 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.6/437.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "pip install -q langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIS_QsnUvjMM",
        "outputId": "b4a43188-c8b8-4d08-85ef-521a42bbdd77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quantum mechanics is the branch of physics that describes the behavior of particles at the smallest scales, where traditional Newtonian physics no longer applies and instead the principles of uncertainty, superposition, and entanglement govern their actions.\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI()\n",
        "output = llm.invoke('Explain quatum mechanics in one sentence.', model='gpt-3.5-turbo')\n",
        "print(output.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PscxhEOwRfL"
      },
      "outputs": [],
      "source": [
        "# help(ChatOpenAI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kk9R2pGJHFYA",
        "outputId": "def4a911-acce-4f31-8190-ba5cce608661"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quantenmechanik ist ein physikalisches Theoriegebäude, das das Verhalten von Teilchen auf atomarer und subatomarer Ebene beschreibt.\n"
          ]
        }
      ],
      "source": [
        "from langchain.schema import SystemMessage, AIMessage, HumanMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content='You are a physicist and respond only in German.'),\n",
        "    HumanMessage(content='Explain quatum mechanics in one sentence.')\n",
        "]\n",
        "\n",
        "output = llm.invoke(messages)\n",
        "print(output.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxWe5ONwQfdx"
      },
      "source": [
        "# Caching LLM Responses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZKnQYDwuAEu"
      },
      "source": [
        "## In-Memory Cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "e_VfLoCvvQA1",
        "outputId": "4abbcecd-cd86-4c87-a8c0-e8a1501c56f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.23-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.56 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.58)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.24 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.24)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.39)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain_community) (2.11.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.56->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.56->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.56->langchain_community) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.56->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain_community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_community-0.3.23-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain_community-0.3.23 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "pip install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DipWjEBbuDHd"
      },
      "outputs": [],
      "source": [
        "from langchain.globals import set_llm_cache\n",
        "from langchain_openai import OpenAI\n",
        "llm = OpenAI(model_name='gpt-3.5-turbo-instruct') # To make the caching really obvious, lets use a slower and older model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "-qawM09WueUt",
        "outputId": "4e2b5632-8e6e-4257-b985-e5e7180e2c56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 1.07 s, sys: 56.1 ms, total: 1.13 s\n",
            "Wall time: 2.19 s\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nWhy was the broom late for school? Because it overswept!'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "from langchain.cache import InMemoryCache\n",
        "set_llm_cache(InMemoryCache())\n",
        "prompt = 'Tell me a joke that a toddler can understand.'\n",
        "llm.invoke(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "uFevPhGOuuKl",
        "outputId": "262942ff-6cf9-462a-b1fa-7043e68b5401"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 1.12 ms, sys: 1e+03 ns, total: 1.12 ms\n",
            "Wall time: 5.09 ms\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nWhy was the broom late for school? Because it overswept!'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "llm.invoke(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHDg1Virwe5y"
      },
      "source": [
        "## SQLite Cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "of5jmSPBwpuK"
      },
      "outputs": [],
      "source": [
        "from langchain.cache import SQLiteCache\n",
        "set_llm_cache(SQLiteCache(database_path='langchaind.db'))\n",
        "\n",
        "# First request (not in cache, takes longer)\n",
        "llm.invoke('Tell me a joke')\n",
        "\n",
        "# Second request (cached, faster)\n",
        "llm.invoke('Tell me a joke')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_1VUd5_1KL_"
      },
      "source": [
        "# LLM Streaming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "q3pEq8qY1OIp",
        "outputId": "4c68496e-6dfa-4973-e6d9-045cda77b373"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verse 1:\n",
            "In the still of the night,\n",
            "Underneath the moon's soft light,\n",
            "I hear the raven's cry,\n",
            "Telling stories of the sky.\n",
            "\n",
            "Chorus:\n",
            "Oh, Moon and Raven,\n",
            "Dancing in the night,\n",
            "Mystical and enchanting,\n",
            "In their own moonlit delight.\n",
            "\n",
            "Verse 2:\n",
            "The raven flies high,\n",
            "As the moon watches from the sky,\n",
            "Their spirits intertwined,\n",
            "In this cosmic design.\n",
            "\n",
            "Chorus:\n",
            "Oh, Moon and Raven,\n",
            "Dancing in the night,\n",
            "Mystical and enchanting,\n",
            "In their own moonlit delight.\n",
            "\n",
            "Bridge:\n",
            "The moon whispers secrets,\n",
            "As the raven sings its song,\n",
            "Together they create a symphony,\n",
            "That will last all night long.\n",
            "\n",
            "Chorus:\n",
            "Oh, Moon and Raven,\n",
            "Dancing in the night,\n",
            "Mystical and enchanting,\n",
            "In their own moonlit delight.\n",
            "\n",
            "Outro:\n",
            "As the night fades away,\n",
            "The moon and raven will stay,\n",
            "Forever connected in the night,\n",
            "In a never-ending flight.\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI()\n",
        "prompt = 'Write a rock song about the Moon and a Raven.'\n",
        "print(llm.invoke(prompt).content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9aQvpdt1_3w",
        "outputId": "433155f6-706b-4ce2-ffb6-4935d8b93e21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verse 1:\n",
            "In the dead of night, the moon appears so bright\n",
            "Casting shadows on the land below\n",
            "And in the darkness, a raven takes flight\n",
            "Singing a song only he knows\n",
            "\n",
            "Chorus:\n",
            "Oh moon, shining bright\n",
            "Guide the raven through the night\n",
            "Oh raven, take flight\n",
            "Sing your song under the moonlight\n",
            "\n",
            "Verse 2:\n",
            "The raven soars high, against the midnight sky\n",
            "Whispering secrets to the moon\n",
            "As the world sleeps, they keep their watchful eye\n",
            "In this cosmic dance, they're in tune\n",
            "\n",
            "Chorus:\n",
            "Oh moon, shining bright\n",
            "Guide the raven through the night\n",
            "Oh raven, take flight\n",
            "Sing your song under the moonlight\n",
            "\n",
            "Bridge:\n",
            "They're two creatures of the night\n",
            "Bound by a mysterious light\n",
            "A dance of shadows in the dark\n",
            "Their connection leaves its mark\n",
            "\n",
            "Chorus:\n",
            "Oh moon, shining bright\n",
            "Guide the raven through the night\n",
            "Oh raven, take flight\n",
            "Sing your song under the moonlight\n",
            "\n",
            "Outro:\n",
            "In the stillness of the night, they roam\n",
            "The moon and raven, forever entwined\n",
            "A haunting melody in the silent gloam\n",
            "Their spirits intertwined."
          ]
        }
      ],
      "source": [
        "for chunk in llm.stream(prompt):\n",
        "  print(chunk.content, end='', flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNMTFLqt64qf"
      },
      "source": [
        "# PromptTemplates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "pQIzUi50CHQM",
        "outputId": "3e9e166a-58fc-4a3d-da84-af0c14b8c9c9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'You are an experience virologist.\\nWrite a few sentences about the following virus \"hiv\" in german.'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "template = '''You are an experience virologist.\n",
        "Write a few sentences about the following virus \"{virus}\" in {language}.'''\n",
        "\n",
        "prompt_template = PromptTemplate(template=template)\n",
        "\n",
        "prompt = prompt_template.format(virus='hiv', language='german')\n",
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLq58_QfDOlJ",
        "outputId": "cc36afcc-d1a3-41e5-9755-0a1d7ffbe9af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HIV, das humane Immundefizienzvirus, ist ein Virus, das das Immunsystem des Menschen schwächt. Es wird hauptsächlich durch ungeschützten Geschlechtsverkehr, den Austausch von infizierten Nadeln oder von der Mutter auf das Kind während der Schwangerschaft übertragen. HIV kann zu AIDS führen, einer schweren Erkrankung, die das Immunsystem so schwächt, dass der Körper anfällig für verschiedene Infektionen und Krankheiten wird. Es ist wichtig, sich über die Übertragungswege von HIV zu informieren und sich vor einer Infektion zu schützen.\n"
          ]
        }
      ],
      "source": [
        "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
        "output = llm.invoke(prompt).content\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYbQmEy2Dt1l"
      },
      "source": [
        "# ChatPromptTemplates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68LxmgzwEbRC",
        "outputId": "93c4b9a0-8adb-4be7-df3d-433e9e145487"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SystemMessage(content='You respond only in the JSON format.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Top 10 countries in World by population.', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "chat_template = ChatPromptTemplate(\n",
        "    [\n",
        "        SystemMessage(content='You respond only in the JSON format.'),\n",
        "        HumanMessagePromptTemplate.from_template('Top {n} countries in {area} by population.')\n",
        "    ]\n",
        ")\n",
        "\n",
        "messages = chat_template.format_messages(n='10', area='World')\n",
        "print(messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffo8MocRGePH",
        "outputId": "35d284c7-0e56-4b84-ab07-ad4e1b2abb1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"1\": {\n",
            "        \"country\": \"China\",\n",
            "        \"population\": \"1,415,045,928\"\n",
            "    },\n",
            "    \"2\": {\n",
            "        \"country\": \"India\",\n",
            "        \"population\": \"1,354,051,854\"\n",
            "    },\n",
            "    \"3\": {\n",
            "        \"country\": \"United States\",\n",
            "        \"population\": \"326,766,748\"\n",
            "    },\n",
            "    \"4\": {\n",
            "        \"country\": \"Indonesia\",\n",
            "        \"population\": \"266,794,980\"\n",
            "    },\n",
            "    \"5\": {\n",
            "        \"country\": \"Pakistan\",\n",
            "        \"population\": \"200,813,818\"\n",
            "    },\n",
            "    \"6\": {\n",
            "        \"country\": \"Brazil\",\n",
            "        \"population\": \"190,732,694\"\n",
            "    },\n",
            "    \"7\": {\n",
            "        \"country\": \"Nigeria\",\n",
            "        \"population\": \"185,989,640\"\n",
            "    },\n",
            "    \"8\": {\n",
            "        \"country\": \"Bangladesh\",\n",
            "        \"population\": \"157,826,578\"\n",
            "    },\n",
            "    \"9\": {\n",
            "        \"country\": \"Russia\",\n",
            "        \"population\": \"142,257,519\"\n",
            "    },\n",
            "    \"10\": {\n",
            "        \"country\": \"Mexico\",\n",
            "        \"population\": \"129,163,276\"\n",
            "    }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI()\n",
        "output = llm.invoke(messages).content\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clOOnGO-HI6Z"
      },
      "source": [
        "# Simple Chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaMA0f-zYE39"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "llm = ChatOpenAI()\n",
        "template = '''You are an experience virologist.\n",
        "Write a few sentences about the following virus \"{virus}\" in {language}.'''\n",
        "prompt_template = PromptTemplate.from_template(template=template)\n",
        "\n",
        "chain = prompt_template | llm | StrOutputParser()\n",
        "\n",
        "output = chain.invoke({'virus': 'HSV', 'language': 'Spanish'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8nu5eI0Zy5V",
        "outputId": "ffe626b2-de3b-4b77-9045-ee361fd9c583"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El virus del herpes simple (HSV) es un virus altamente contagioso que afecta principalmente a la piel y las membranas mucosas. Hay dos tipos de HSV: el tipo 1, que suele causar herpes labial en la boca y alrededor de los labios, y el tipo 2, que suele causar herpes genital. Ambos tipos de HSV son crónicos y pueden producir brotes recurrentes a lo largo de la vida de una persona infectada. Es importante practicar una buena higiene y evitar el contacto directo con lesiones activas para prevenir la transmisión del virus.\n"
          ]
        }
      ],
      "source": [
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXYBMP5KZ00u",
        "outputId": "20afb6bf-1154-48fd-e806-dd41fadf5768"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Country: Brazil\n",
            "The capital of Brazil is Brasília.\n",
            "\n",
            "- Juscelino Kubitschek Memorial: This museum pays tribute to Juscelino Kubitschek, the president who established Brasília as the country's capital.\n",
            "- National Congress: This iconic building houses both the Chamber of Deputies and the Federal Senate of Brazil, and is a must-visit for anyone interested in politics.\n",
            "- Planalto Palace: This is the official workplace of the President of Brazil and is an important landmark in Brasília.\n"
          ]
        }
      ],
      "source": [
        "template = 'What is the capital of {country}?. List the top 3 places to visit in that city. Use bullet points.'\n",
        "prompt_template = PromptTemplate.from_template(template=template)\n",
        "\n",
        "chain = prompt_template | llm | StrOutputParser()\n",
        "\n",
        "country = input('Enter Country: ')\n",
        "output = chain.invoke(country)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbumTr9EbDjK"
      },
      "source": [
        "# Sequential Chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPmxW8Bjc9uQ"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain import PromptTemplate\n",
        "# from langchain.chains import SimpleSequentialChain\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "llm1 = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5)\n",
        "prompt_template1 = PromptTemplate.from_template(\n",
        "    template='You are an experienced scientist and Python programmer. Write a function that implements the concept of {concept}.'\n",
        ")\n",
        "chain1 = prompt_template1 | llm1 | StrOutputParser()\n",
        "\n",
        "llm2 = ChatOpenAI(model_name='gpt-4-turbo-preview', temperature=1.2)\n",
        "prompt_template2 = PromptTemplate.from_template(\n",
        "    template = 'Given the Python function {function}, describe it as detailed as possible.'\n",
        ")\n",
        "chain2 = prompt_template2 | llm2 | StrOutputParser()\n",
        "\n",
        "overall_chain = chain1 | chain2\n",
        "output = overall_chain.invoke('linear regression')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vpzbpsd-2dTh",
        "outputId": "7fa1adc1-d7f2-4557-c410-4da2f1aa8d93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Python function `linear_regression` is designed to perform linear regression, a fundamental method in statistical modeling and machine learning. Linear regression aims to model the linear relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data. The goal is to find the coefficients (or parameters) of the linear equation that best predict the dependent variable. Here, the function focuses on simple linear regression, where there is just one independent variable, though the method can be slightly adjusted to handle multiple variables. Below is a detailed description of the function’s implementation and its contents.\n",
            "\n",
            "### Importing Necessary Libraries\n",
            "\n",
            "The function begins by importing NumPy, a widely used library in Python for numerical computing. NumPy provides efficient data structures for arrays and matrices, along with a collection of mathematical functions to operate on these data structures, which is essential for implementing linear regression from scratch.\n",
            "\n",
            "```python\n",
            "import numpy as np\n",
            "```\n",
            "\n",
            "### Function Definition\n",
            "\n",
            "The `linear_regression` function is defined with two parameters: `X` and `y`.\n",
            "- `X`: This is a matrix (or two-dimensional NumPy array) representing the input features. In simple linear regression (as illustrated in the example), this consists of a single column where each value represents one observation's feature value.\n",
            "- `y`: This is a vector (or one-dimensional NumPy array) representing the target values. Each entry in `y` corresponds to the output value for each observation in `X`.\n",
            "\n",
            "```python\n",
            "def linear_regression(X, y):\n",
            "```\n",
            "\n",
            "### Adding an Intercept Term\n",
            "\n",
            "Linear regression models attempt to predict an output as a linear combination of input features. The equation for simple linear regression can be expressed as `y = mx + b`, where `b` (the y-intercept) plays a crucial role. To account for this intercept in the matrix framework, the function adds a column of ones to the matrix `X`. This allows the intercept (denoted as `b` in the simple equation) to be included in the matrix multiplication process, treating it as an additional coefficient.\n",
            "\n",
            "```python\n",
            "X = np.c_[np.ones(X.shape[0]), X]\n",
            "```\n",
            "\n",
            "### Computing the Coefficients\n",
            "\n",
            "The most computationally efficient method to find the regression coefficients that minimize the square differences between observed and predicted values is through the normal equation:\n",
            "\\[ coefficients = (X^T X)^{-1} X^T y \\]\n",
            "This closed-form solution directly computes the coefficients (`m` and `b` in simple regression) as follows:\n",
            "- `X.T` is the transpose of matrix `X`, flipping its dimensions.\n",
            "- `.dot()` is a method for matrix multiplication.\n",
            "- `np.linalg.inv()` calculates the inverse of a matrix.\n",
            "\n",
            "By applying this formula, the function calculates the coefficients that best fit the linear model to the given data.\n",
            "\n",
            "```python\n",
            "coefficients = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
            "```\n",
            "\n",
            "### Return Values\n",
            "\n",
            "The function returns the calculated coefficients. In the context of simple linear regression, the returned value will consist of two elements:\n",
            "- The intercept `b`, which is the first element of the coefficients array.\n",
            "- The slope `m`, which is the second element. This represents the amount of change in the predicted output `y` for a one-unit change in the input `x`.\n",
            "\n",
            "```python\n",
            "return coefficients\n",
            "```\n",
            "\n",
            "### Example Usage\n",
            "\n",
            "An example usage scenario demonstrates how to use the function with input features `X` (consisting of a single feature for simplicity) and the target values `y`. The provided data represents a linear relationship (`y = 2x + 0`), so the output of the `linear_regression` function will be the estimated coefficients, effectively identifying the underlying relationship as expected (an intercept of `0` and a slope of `2`).\n",
            "\n",
            "```python\n",
            "X = np.array([[1], [2], [3], [4], [5]])\n",
            "y = np.array([2, 4, 6, 8, 10])\n",
            "\n",
            "coefficients = linear_regression(X, y)\n",
            "print(\"Intercept:\", coefficients[0])\n",
            "print(\"Slope:\", coefficients[1])\n",
            "```\n",
            "\n",
            "This function demonstrates a straightforward implementation of linear regression, providing a solid foundation for understanding and extending to more complex scenarios, such as multiple linear regression (with modifications) or incorporating regularization techniques to prevent overfitting.\n"
          ]
        }
      ],
      "source": [
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmAgWxXOGkA9"
      },
      "source": [
        "# LangChain Agents in Action: Python REPL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zo32ONCGGnPf",
        "outputId": "4b63aeaa-130e-41d8-9c6e-4d892586a85d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/209.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m204.8/209.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "pip install -q langchain_experimental"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "u_o9Kuu3HT-2",
        "outputId": "c63a8fe7-c719-440e-b29e-835db513b47a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_experimental.utilities.python:Python REPL can execute arbitrary code. Use with caution.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[13, 26, 39, 52, 65, 78, 91]\\n'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_experimental.utilities import PythonREPL\n",
        "python_repl = PythonREPL()\n",
        "python_repl.run('print([n for n in range(1, 100) if n % 13 == 0])')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cyoffg9vHtkk",
        "outputId": "ec83fbc5-12ad-462a-dc14-8e6a540821be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo solve this, I need to calculate the factorial of 12 first, then find its square root, and finally format the result to display it with 4 decimal points. I can use the `math` module in Python for both the factorial and square root calculations.\n",
            "Action: Python_REPL\n",
            "Action Input: import math\n",
            "print(f\"{math.sqrt(math.factorial(12)):.4f}\")\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m21886.1052\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "Final Answer: 21886.1052\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'Calculate the square root of the factorial of 12 and display it with 4 decimal points',\n",
              " 'output': '21886.1052'}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
        "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model_name='gpt-4-turbo-preview', temperature=0)\n",
        "agent_executor = create_python_agent(\n",
        "    llm=llm,\n",
        "    tool=PythonREPLTool(),\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "agent_executor.invoke('Calculate the square root of the factorial of 12 and display it with 4 decimal points')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myZWsoJaIRDt",
        "outputId": "7da19018-0dfb-4b39-a699-55ded1ccc91f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI need to calculate 5.1 raised to the power of 7.3 to get the answer.\n",
            "Action: Python_REPL\n",
            "Action Input: print(5.1 ** 7.3)\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m146306.05007233328\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "Final Answer: 146306.05007233328\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "response = agent_executor.invoke('What is the answer to 5.1 ** 7.3?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vV32uuyMkyk",
        "outputId": "dc2f0877-7dd9-4a15-b1f2-85766ee78767"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input': 'What is the answer to 5.1 ** 7.3?', 'output': '146306.05007233328'}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GofV7H9oM4nH",
        "outputId": "987a6888-ff74-49b5-92c2-aee503fdae2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "146306.05007233328\n"
          ]
        }
      ],
      "source": [
        "print(response['output'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YTik3IAM-AD"
      },
      "source": [
        "# LangChain Tools: DuckDuckGo and Wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cS54241qTBZ-",
        "outputId": "a6c5ff59-cc93-4c31-a0d6-482404eed9f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/3.3 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "pip install -q duckduckgo-search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iN2QuGpWTug8",
        "outputId": "7300d973-f59c-48e2-b9fb-a267f6ae88b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Freddie Mercury (born September 5, 1946, Stone Town, Zanzibar [now in Tanzania]—died November 24, 1991, Kensington, London, England) was a British rock singer and songwriter whose flamboyant showmanship and powerfully agile vocals, most famously for the band Queen, made him one of rock's most dynamic front men.. Bulsara was born to Parsi parents who had emigrated from India to Zanzibar ... Freddie Mercury, born Farrokh Bulsara on September 5, 1946, in Zanzibar, was a legendary singer-songwriter and the charismatic frontman of the rock band Queen. With a unique background as a Parsi in Tanzania and a music education in India, Mercury made a significant impact on the music world. Freddie Mercury was born Farrokh Bulsara in Stone Town in the British protectorate of the Sultanate of Zanzibar, East Africa (now part of Tanzania) on September 5, 1946. When did he get his start in music? Freddie Mercury began his musical career in the late 1960s. He formed his first band, The Hectics, while attending St. Peter's School in ... Freddie Mercury, born Farrokh Bulsara on September 5, 1946, in Stone Town, Zanzibar (now part of Tanzania), was a British singer, songwriter, and record producer. Renowned as the flamboyant frontman of the rock band Queen, Mercury's powerful vocals and dynamic stage presence cemented his status as one of rock music's most iconic figures. Freddie Mercury was born Farrokh Bulsara on September 5, 1946, in Stone Town, Zanzibar. His early years in the vibrant island city were influenced by its diverse culture and rich history. His parents, Bomi and Jer Bulsara, were Parsi, and they hailed from India. This multicultural environment played a significant role in shaping Mercury's ...\n"
          ]
        }
      ],
      "source": [
        "from langchain.tools import DuckDuckGoSearchRun\n",
        "\n",
        "search = DuckDuckGoSearchRun()\n",
        "output = search.invoke('Where was Freddie Mercury born?')\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "M245CzrTUM47",
        "outputId": "aafa8c7a-d9ea-4f07-a40d-91b347779154"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'duckduckgo_search'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "search.name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QE0Ehm0aUg8v",
        "outputId": "56f01bae-341f-47c2-97e6-a11ceb3b4233"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "search.description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91vNSJRpUikR",
        "outputId": "2647dad3-0995-4a8e-817b-eb59579bea2f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DuckDuckGoSearchRun(api_wrapper=DuckDuckGoSearchAPIWrapper(region='wt-wt', safesearch='moderate', time='y', max_results=5, backend='auto', source='text'))"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itH6H2XYUqS_",
        "outputId": "6c89806c-42de-480f-a483-e105cb27ebca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "snippet: Freddie Mercury (born September 5, 1946, Stone Town, Zanzibar [now in Tanzania]—died November 24, 1991, Kensington, London, England) was a British rock singer and songwriter whose flamboyant showmanship and powerfully agile vocals, most famously for the band Queen, made him one of rock's most dynamic front men.. Bulsara was born to Parsi parents who had emigrated from India to Zanzibar ..., title: Freddie Mercury | Biography, Parents, Songs, & Facts | Britannica, link: https://www.britannica.com/biography/Freddie-Mercury, snippet: When Freddie Mercury was looking to break free of Queen as his band went on hiatus in 1983, he turned to a king — King of Pop, Michael Jackson. The two late music legends were mutual fans ..., title: Exclusive | Why Freddie Mercury never finished 3 duets with Michael ..., link: https://nypost.com/2025/04/28/entertainment/why-freddie-mercury-never-finished-3-duets-with-michael-jackson-it-involves-a-llama/, snippet: Freddie Mercury of Queen, interview and photo session for 'Music Life' magazine, on the band's Hot Space Japan tour at a hotel in Fukuoka, Japan, 19 October 1982. It was the band's fifth visit to Japan. (Photo by Midori Tsukagoshi/ShinkoMusic/Getty Images), title: The Truth Behind Freddie Mercury's Relationship With His Queen Bandmates, link: https://societyofrock.com/the-truth-behind-freddie-mercurys-relationship-with-his-queen-bandmates/, snippet: Freddie Mercury's journey with Queen began in the early 1970s when he met guitarist Brian May and drummer Roger Taylor. After forming a friendship and musical partnership, they soon added bassist John Deacon to the mix, officially establishing the band in 1971. Mercury, with his dynamic stage presence and unique vocal abilities, quickly became ..., title: Freddie Mercury - Singer-Songwriter, Age, Married, Children, Wife, link: https://biographyhost.com/p/freddie-mercury-biography.html\n"
          ]
        }
      ],
      "source": [
        "from langchain.tools import DuckDuckGoSearchResults\n",
        "\n",
        "search = DuckDuckGoSearchResults()\n",
        "output = search.run('Freddie Mercury and Queen')\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dMReWvVU8-g",
        "outputId": "83e10af7-016a-42aa-b918-d9a8b241a1eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "snippet: Erfahren Sie mehr über die Geschichte, Geographie, Politik und Kultur der bevölkerungsreichsten Stadt Deutschlands und der Europäischen Union. Lesen Sie die wichtigsten Fakten, Daten und Sehenswürdigkeiten Berlins auf der umfassenden Online-Enzyklopädie., title: Berlin - Wikipedia, link: https://de.wikipedia.org/wiki/Berlin, snippet: Der Stadtführer für Berlin mit aktuellen Informationen und Auskunft zu Jobs, zum Leben, Arbeiten, Ausgehen, Einkaufen und Urlauben in Berlin., title: Stadtportal Berlin | Stadtinformationen für Berlin bei ... - meinestadt.de, link: https://home.meinestadt.de/berlin, snippet: Einfach zurechtfinden: Mit unserer übersichtlichen Liste der Sehenswürdigkeiten in Berlin planst du deinen Städtetrip ohne Umwege., title: Berlin Sehenswürdigkeiten: Top-25-Liste mit Tipps & Bezirken, link: https://www.voucherwonderland.com/reisemagazin/top-20-sehenswuerdigkeiten-in-berlin/, snippet: Du fragst dich, was man in Berlin machen kann? Wir zeigen dir die besten Tagesausflüge, Unternehmungen & Aktivitäten für Berlin!, title: Was kann man in Berlin machen? TOP 25 Aktivitäten & Unternehmungen!, link: https://www.traveloptimizer.de/ausflugsziele-berlin/\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
        "\n",
        "wrapper = DuckDuckGoSearchAPIWrapper(region='de-de', max_results=3, safesearch='moderate')\n",
        "search = DuckDuckGoSearchResults(api_wrapper=wrapper, source='news')\n",
        "output = search.run('Berlin')\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9YetkZrXG2I",
        "outputId": "5abe2b6e-f63d-4a79-d8ed-32ffdd368d35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Snippet: Erfahren Sie mehr über die Geschichte, Geographie, Politik und Kultur der bevölkerungsreichsten Stadt Deutschlands und der Europäischen Union. Lesen Sie die wichtigsten Fakten, Daten und Sehenswürdigkeiten Berlins auf der umfassenden Online-Enzyklopädie.\n",
            "Title: Berlin - Wikipedia\n",
            "Link: https://de.wikipedia.org/wiki/Berlin\n",
            "\n",
            "--------------------------------------------------\n",
            "Snippet: Der Stadtführer für Berlin mit aktuellen Informationen und Auskunft zu Jobs, zum Leben, Arbeiten, Ausgehen, Einkaufen und Urlauben in Berlin.\n",
            "Title: Stadtportal Berlin | Stadtinformationen für Berlin bei ... - meinestadt.de\n",
            "Link: https://home.meinestadt.de/berlin\n",
            "\n",
            "--------------------------------------------------\n",
            "Snippet: Einfach zurechtfinden: Mit unserer übersichtlichen Liste der Sehenswürdigkeiten in Berlin planst du deinen Städtetrip ohne Umwege.\n",
            "Title: Berlin Sehenswürdigkeiten: Top-25-Liste mit Tipps & Bezirken\n",
            "Link: https://www.voucherwonderland.com/reisemagazin/top-20-sehenswuerdigkeiten-in-berlin/\n",
            "\n",
            "--------------------------------------------------\n",
            "Snippet: Du fragst dich, was man in Berlin machen kann? Wir zeigen dir die besten Tagesausflüge, Unternehmungen & Aktivitäten für Berlin!\n",
            "Title: Was kann man in Berlin machen? TOP 25 Aktivitäten & Unternehmungen!\n",
            "Link: https://www.traveloptimizer.de/ausflugsziele-berlin/\n",
            "\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "pattern = r'snippet: (.*?), title: (.*?), link: (.*?)(?:,|$)'\n",
        "matches = re.findall(pattern, output, re.DOTALL)\n",
        "\n",
        "for snippet, title, link in matches:\n",
        "  print(f'Snippet: {snippet}\\nTitle: {title}\\nLink: {link}\\n')\n",
        "  print('-' * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zOf4_E7YXU5",
        "outputId": "ce7daf23-af54-4861-aeab-3bd772c95292"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "pip install -q wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "M9dvXFt3aSYz",
        "outputId": "0062035e-e969-4d1e-aeaf-ac9f9e35f8d8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Page: Vector database\\nSummary: A vector database, vector store or vector search engine is a database that can store vectors (fixed-length lists of numbers) along with other data items. Vector databases typically implement one or more Approximate Nearest Neighbor algorithms, so that one can search the database with a query vector to retrieve the closest matching database records.\\nVectors are mathematical representations of data in a high-dimensional space. In this space, each dimension corresponds to a feature of the data, with the number of dimensions ranging from a few hundred to tens of thousands, depending on the complexity of the data being represented. A vector\\'s position in this space represents its characteristics. Words, phrases, or entire documents, as well as images, audio, and other types of data, can all be vectorized.\\nThese feature vectors may be computed from the raw data using machine learning methods such as feature extraction algorithms, word embeddings or deep learning networks. The goal is that semantically similar data items receive feature vectors close to each other.\\nVector databases can be used for similarity search, semantic search, multi-modal search, recommendations engines, large language models (LLMs), object detection,  etc.\\nVector databases are also often used to implement retrieval-augmented generation (RAG), a method to improve domain-specific responses of large language models. The retrieval component of a RAG can be any search system, but is most often implemented as a vector database. Text documents describing the domain of interest are collected, and for each document or document section, a feature vector (known as an \"embedding\") is computed, typically using a deep learning network, and stored in a vector database. Given a user prompt, the feature vector of the prompt is computed, and the database is queried to retrieve the most relevant documents. These are then automatically added into the context window of the large language model, and the large language model proceeds to create a response to the prompt given this context.'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "\n",
        "api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=5000)\n",
        "wiki = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
        "wiki.invoke({'query': 'vector database'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "eME-FCXja5nz",
        "outputId": "53e76898-5dca-4652-e9cf-39102f2a7285"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Page: Gemini (chatbot)\\nSummary: Gemini, formerly known as Bard, is a generative artificial intelligence chatbot developed by Google. Based on the large language model (LLM) of the same name, it was launched in 2023 in response to the rise of OpenAI\\'s ChatGPT. It was previously based on the LaMDA and PaLM LLMs.\\nLaMDA had been developed and announced in 2021, but it was not released to the public out of an abundance of caution. OpenAI\\'s launch of ChatGPT in November 2022 and its subsequent popularity caught Google executives off-guard, prompting a sweeping response in the ensuing months. After mobilizing its workforce, the company launched Bard in a limited capacity in March 2023 before expanding to other countries in May. Bard took center stage during the 2023 Google I/O keynote in May and was upgraded to the Gemini LLM in December. In February 2024, Bard and Duet AI, another artificial intelligence product from Google, were unified under the Gemini brand, coinciding with the launch of an Android app.\\nGemini initially received lukewarm responses. It became the center of controversy in February 2024, when social media users reported that it was generating historically inaccurate images of historical figures as people of color, with people commenting on its bias as \"wokeness\".\\n\\n'"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wiki.invoke('Google Gemini')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXCBKCldb-H3"
      },
      "source": [
        "# Creating a React Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "aezFncasAqg6",
        "outputId": "a1c41e1c-c9ef-4204-f5fa-fdb652b99e57"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-19-82c9e354a355>, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-82c9e354a355>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip install langchainhub -q\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "pip install langchainhub -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SE0VfvLAzjP",
        "outputId": "44c88d9e-ff4f-4c00-925e-99271f29ec94"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "load_dotenv(find_dotenv(), override=True)\n",
        "\n",
        "# os.environ.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMGCYQuyISNy",
        "outputId": "10cb1cb9-8d39-4ed2-9be2-a790bd676091"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m2.3/3.3 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "pip install -q duckduckgo-search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OftfV8h3Ia1O",
        "outputId": "230fc836-e733-4b2a-934a-586de077cfb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "pip install wikipedia -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gs6qG0JFBA5v",
        "outputId": "6e791e9b-db31-49ad-d811-02c0f7292d42"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain import hub\n",
        "from langchain.agents import Tool, AgentExecutor, initialize_agent, create_react_agent\n",
        "from langchain.tools import DuckDuckGoSearchRun, WikipediaQueryRun\n",
        "from langchain.utilities import WikipediaAPIWrapper\n",
        "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model_name='gpt-4-turbo-preview', temperature=0) # gpt-3.5-turbo for free access\n",
        "\n",
        "# Template\n",
        "template = '''\n",
        "Answer the following questions in GERMAN as best you can.\n",
        "Questions: {q}\n",
        "'''\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(template)\n",
        "prompt = hub.pull('hwchase17/react')\n",
        "# print(type(prompt))\n",
        "# print(prompt.input_variables)\n",
        "# print(prompt.template)\n",
        "\n",
        "# 1. Python REPL Tool (for executing Python code)\n",
        "python_repl = PythonREPLTool()\n",
        "python_repl_tool = Tool(\n",
        "    name='Python REPL',\n",
        "    func=python_repl.run,\n",
        "    description='Useful when you need to use Python to answer a question. You should input Python code.'\n",
        ")\n",
        "\n",
        "# 2. Wikipedia Tool (for searching Wikipedia)\n",
        "api_wrapper = WikipediaAPIWrapper()\n",
        "wikipedia = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
        "wikipedia_tool = Tool(\n",
        "    name='Wikipedia',\n",
        "    func=wikipedia.run,\n",
        "    description='Useful for when you need to look up a topic, country, or person on Wikipedia.'\n",
        ")\n",
        "\n",
        "# 3. DuckDuckGo Search Tool (for general web searches)\n",
        "search = DuckDuckGoSearchRun()\n",
        "duckduckgo_tool = Tool(\n",
        "    name='DuckDuckGo Search',\n",
        "    func=search.run,\n",
        "    description='Useful when you need to perform an internet search to find information that another tool can\\'t provide.'\n",
        ")\n",
        "\n",
        "# Tools\n",
        "tools = [python_repl_tool, wikipedia_tool, duckduckgo_tool]\n",
        "\n",
        "# Agent\n",
        "agent = create_react_agent(llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=tools,\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True,\n",
        "    max_iterations=10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCBVWbOpDap0",
        "outputId": "6dc7fbb7-3208-4044-c2c1-db36aba14e76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo answer this question in German, I will first find information about Napoleon Bonaparte's early life in English and then translate it into German.\n",
            "\n",
            "Action: Wikipedia\n",
            "Action Input: Napoleon Bonaparte\u001b[0m\u001b[33;1m\u001b[1;3mPage: Napoleon\n",
            "Summary: Napoleon Bonaparte (born Napoleone Buonaparte; 15 August 1769 – 5 May 1821), later known by his regnal name Napoleon I, was a French general and statesman who rose to prominence during the French Revolution and led a series of military campaigns across Europe during the French Revolutionary and Napoleonic Wars from 1796 to 1815. He led the French Republic as First Consul from 1799 to 1804, then ruled the French Empire as Emperor of the French from 1804 to 1814, and briefly again in 1815. He was King of Italy from 1805 to 1814 and Protector of the Confederation of the Rhine from 1806 to 1813.\n",
            "Born on the island of Corsica to a family of Italian origin, Napoleon moved to mainland France in 1779 and was commissioned as an officer in the French Royal Army in 1785. He supported the French Revolution in 1789 and promoted its cause in Corsica. He rose rapidly through the ranks after winning the siege of Toulon in 1793 and defeating royalist insurgents in Paris on 13 Vendémiaire in 1795. In 1796, Napoleon commanded a military campaign against the Austrians and their Italian allies in the War of the First Coalition, scoring decisive victories and becoming a national hero. He led an invasion of Egypt and Syria in 1798 which served as a springboard to political power. In November 1799, Napoleon engineered the Coup of 18 Brumaire against the French Directory and became First Consul of the Republic. He won the Battle of Marengo in 1800, which secured France's victory in the War of the Second Coalition, and in 1803 he sold the territory of Louisiana to the United States. In December 1804, Napoleon crowned himself Emperor of the French, further expanding his power.\n",
            "The breakdown of the Treaty of Amiens led to the War of the Third Coalition by 1805. Napoleon shattered the coalition with a decisive victory at the Battle of Austerlitz, which led to the dissolution of the Holy Roman Empire. In the War of the Fourth Coalition, Napoleon defeated Prussia at the Battle of Jena–Auerstedt in 1806, marched his Grande Armée into Eastern Europe, and defeated the Russians in 1807 at the Battle of Friedland. Seeking to extend his trade embargo against Britain, Napoleon invaded the Iberian Peninsula and installed his brother Joseph as King of Spain in 1808, provoking the Peninsular War. In 1809, the Austrians again challenged France in the War of the Fifth Coalition, in which Napoleon solidified his grip over Europe after winning the Battle of Wagram. In summer 1812, he launched an invasion of Russia, briefly occupying Moscow before conducting a catastrophic retreat of his army that winter. In 1813, Prussia and Austria joined Russia in the War of the Sixth Coalition, in which Napoleon was decisively defeated at the Battle of Leipzig. The coalition invaded France and captured Paris, forcing Napoleon to abdicate in April 1814. They exiled him to the Mediterranean island of Elba and restored the Bourbons to power. Ten months later, Napoleon escaped from Elba on a brig, landed in France with a thousand men, and marched on Paris, again taking control of the country. His opponents responded by forming a Seventh Coalition, which defeated him at the Battle of Waterloo in June 1815. Napoleon was exiled to the remote island of Saint Helena in the South Atlantic, where he died of stomach cancer in 1821, aged 51.\n",
            "Napoleon is considered one of the greatest military commanders in history, and Napoleonic tactics are still studied at military schools worldwide. His legacy endures through the modernizing legal and administrative reforms he enacted in France and Western Europe, embodied in the Napoleonic Code. He established a system of public education, abolished the vestiges of feudalism, emancipated Jews and other religious minorities, abolished the Spanish Inquisition, enacted the principle of equality before the law for an emerging middle class, and centralized state power at the expense of religious authorities. His conquests acted as a catalyst\u001b[0m\u001b[32;1m\u001b[1;3mI have the information about Napoleon Bonaparte's early life. Now, I need to translate the relevant parts into German.\n",
            "\n",
            "Final Answer: Napoleon Bonaparte wurde am 15. August 1769 auf der Insel Korsika in eine Familie italienischer Herkunft geboren. Im Jahr 1779 zog Napoleon nach Frankreich und wurde 1785 als Offizier in die französische Königsarmee aufgenommen. Er unterstützte die Französische Revolution von 1789 und förderte ihre Sache auf Korsika. Durch den Sieg bei der Belagerung von Toulon im Jahr 1793 und die Niederlage der royalistischen Aufständischen in Paris am 13. Vendémiaire im Jahr 1795 stieg er schnell in den militärischen Rängen auf. Im Jahr 1796 befehligte Napoleon eine militärische Kampagne gegen die Österreicher und ihre italienischen Verbündeten im Krieg der Ersten Koalition, erzielte entscheidende Siege und wurde zum Nationalhelden.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# question = 'Generate the first 20 numbers in the Fibonacci series.'\n",
        "# question = 'Who is the current prime minister of the UK?'\n",
        "question = 'Tell me about Napoleon Bonaparte early life'\n",
        "\n",
        "output = agent_executor.invoke({\n",
        "    'input': prompt_template.format(q=question)\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7JKTCslMs_v",
        "outputId": "9b43e604-04f4-4c6d-a004-ade158a919f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input \n",
            "Answer the following questions in GERMAN as best you can.\n",
            "Questions: Tell me about Napoleon Bonaparte early life\n",
            "\n",
            "output Napoleon Bonaparte wurde am 15. August 1769 auf der Insel Korsika in eine Familie italienischer Herkunft geboren. Im Jahr 1779 zog Napoleon nach Frankreich und wurde 1785 als Offizier in die französische Königsarmee aufgenommen. Er unterstützte die Französische Revolution von 1789 und förderte ihre Sache auf Korsika. Durch den Sieg bei der Belagerung von Toulon im Jahr 1793 und die Niederlage der royalistischen Aufständischen in Paris am 13. Vendémiaire im Jahr 1795 stieg er schnell in den militärischen Rängen auf. Im Jahr 1796 befehligte Napoleon eine militärische Kampagne gegen die Österreicher und ihre italienischen Verbündeten im Krieg der Ersten Koalition, erzielte entscheidende Siege und wurde zum Nationalhelden.\n"
          ]
        }
      ],
      "source": [
        "for i in output.keys():\n",
        "  print(i, output[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhbwrLyWRXEQ"
      },
      "source": [
        "## Adding a new tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJeHnOzKNSmE",
        "outputId": "6522f3ad-84db-4bb8-8289-dd597dac3654"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/81.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "pip install arxiv -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2MSEP5eRbtl",
        "outputId": "309cbc33-4560-455d-f6cd-c65e72a98d15"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain import hub\n",
        "from langchain.agents import Tool, AgentExecutor, initialize_agent, create_react_agent\n",
        "from langchain.tools import DuckDuckGoSearchRun, WikipediaQueryRun\n",
        "from langchain.utilities import WikipediaAPIWrapper, ArxivAPIWrapper\n",
        "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model_name='gpt-4-turbo-preview', temperature=0) # gpt-3.5-turbo for free access\n",
        "\n",
        "# Template\n",
        "template = '''\n",
        "Answer the following questions in GERMAN as best you can.\n",
        "Questions: {q}\n",
        "'''\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(template)\n",
        "prompt = hub.pull('hwchase17/react')\n",
        "# print(type(prompt))\n",
        "# print(prompt.input_variables)\n",
        "# print(prompt.template)\n",
        "\n",
        "# 1. Python REPL Tool (for executing Python code)\n",
        "python_repl = PythonREPLTool()\n",
        "python_repl_tool = Tool(\n",
        "    name='Python REPL',\n",
        "    func=python_repl.run,\n",
        "    description='Useful when you need to use Python to answer a question. You should input Python code.'\n",
        ")\n",
        "\n",
        "# 2. Wikipedia Tool (for searching Wikipedia)\n",
        "api_wrapper = WikipediaAPIWrapper()\n",
        "wikipedia = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
        "wikipedia_tool = Tool(\n",
        "    name='Wikipedia',\n",
        "    func=wikipedia.run,\n",
        "    description='Useful for when you need to look up a topic, country, or person on Wikipedia.'\n",
        ")\n",
        "\n",
        "# 3. DuckDuckGo Search Tool (for general web searches)\n",
        "search = DuckDuckGoSearchRun()\n",
        "duckduckgo_tool = Tool(\n",
        "    name='DuckDuckGo Search',\n",
        "    func=search.run,\n",
        "    description='Useful when you need to perform an internet search to find information that another tool can\\'t provide.'\n",
        ")\n",
        "\n",
        "# 4. Arxiv Search Tool (for scientific papers)\n",
        "arxiv = ArxivAPIWrapper()\n",
        "arxiv_tool = Tool(\n",
        "    name='Arxiv Search',\n",
        "    func = arxiv.run,\n",
        "    description='Useful for finding scientific papers and abstracts from arXiv. Input should be a research topic.'\n",
        ")\n",
        "\n",
        "# Tools\n",
        "tools = [python_repl_tool, wikipedia_tool, duckduckgo_tool, arxiv_tool]\n",
        "\n",
        "# Agent\n",
        "agent = create_react_agent(llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=tools,\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True,\n",
        "    max_iterations=10\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trzrzk9JSJYT",
        "outputId": "1d1532bd-ad53-4fb2-f88e-09f9ede2ee26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo find the most recent papers about Deep Learning, I should use the Arxiv Search tool since it specializes in finding scientific papers and abstracts.\n",
            "\n",
            "Action: Arxiv Search\n",
            "Action Input: Deep Learning\u001b[0m\u001b[36;1m\u001b[1;3mPublished: 2018-05-22\n",
            "Title: Opening the black box of deep learning\n",
            "Authors: Dian Lei, Xiaoxiao Chen, Jianfei Zhao\n",
            "Summary: The great success of deep learning shows that its technology contains\n",
            "profound truth, and understanding its internal mechanism not only has important\n",
            "implications for the development of its technology and effective application in\n",
            "various fields, but also provides meaningful insights into the understanding of\n",
            "human brain mechanism. At present, most of the theoretical research on deep\n",
            "learning is based on mathematics. This dissertation proposes that the neural\n",
            "network of deep learning is a physical system, examines deep learning from\n",
            "three different perspectives: microscopic, macroscopic, and physical world\n",
            "views, answers multiple theoretical puzzles in deep learning by using physics\n",
            "principles. For example, from the perspective of quantum mechanics and\n",
            "statistical physics, this dissertation presents the calculation methods for\n",
            "convolution calculation, pooling, normalization, and Restricted Boltzmann\n",
            "Machine, as well as the selection of cost functions, explains why deep learning\n",
            "must be deep, what characteristics are learned in deep learning, why\n",
            "Convolutional Neural Networks do not have to be trained layer by layer, and the\n",
            "limitations of deep learning, etc., and proposes the theoretical direction and\n",
            "basis for the further development of deep learning now and in the future. The\n",
            "brilliance of physics flashes in deep learning, we try to establish the deep\n",
            "learning technology based on the scientific theory of physics.\n",
            "\n",
            "Published: 2018-06-05\n",
            "Title: Concept-Oriented Deep Learning\n",
            "Authors: Daniel T Chang\n",
            "Summary: Concepts are the foundation of human deep learning, understanding, and\n",
            "knowledge integration and transfer. We propose concept-oriented deep learning\n",
            "(CODL) which extends (machine) deep learning with concept representations and\n",
            "conceptual understanding capability. CODL addresses some of the major\n",
            "limitations of deep learning: interpretability, transferability, contextual\n",
            "adaptation, and requirement for lots of labeled training data. We discuss the\n",
            "major aspects of CODL including concept graph, concept representations, concept\n",
            "exemplars, and concept representation learning systems supporting incremental\n",
            "and continual learning.\n",
            "\n",
            "Published: 2019-07-30\n",
            "Title: Deep learning research landscape & roadmap in a nutshell: past, present and future -- Towards deep cortical learning\n",
            "Authors: Aras R. Dargazany\n",
            "Summary: The past, present and future of deep learning is presented in this work.\n",
            "Given this landscape & roadmap, we predict that deep cortical learning will be\n",
            "the convergence of deep learning & cortical learning which builds an artificial\n",
            "cortical column ultimately.\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "Final Answer: Die neuesten Papiere über Deep Learning sind:\n",
            "\n",
            "1. Titel: Opening the black box of deep learning\n",
            "   Autoren: Dian Lei, Xiaoxiao Chen, Jianfei Zhao\n",
            "   Veröffentlicht: 22. Mai 2018\n",
            "   Zusammenfassung: Diese Dissertation zeigt, dass die Technologie des Deep Learning tiefe Wahrheiten enthält. Die Untersuchung seines internen Mechanismus hat nicht nur wichtige Implikationen für die Entwicklung seiner Technologie und effektive Anwendung in verschiedenen Bereichen, sondern bietet auch bedeutungsvolle Einblicke in das Verständnis des menschlichen Gehirnmechanismus.\n",
            "\n",
            "2. Titel: Concept-Oriented Deep Learning\n",
            "   Autoren: Daniel T Chang\n",
            "   Veröffentlicht: 5. Juni 2018\n",
            "   Zusammenfassung: Konzepte sind die Grundlage des menschlichen tiefen Lernens, Verstehens und der Wissensintegration und -übertragung. Wir schlagen ein konzeptorientiertes Deep Learning (CODL) vor, das (maschinelles) tiefes Lernen mit Konzeptrepräsentationen und konzeptuellem Verständnis erweitert.\n",
            "\n",
            "3. Titel: Deep learning research landscape & roadmap in a nutshell: past, present and future -- Towards deep cortical learning\n",
            "   Autoren: Aras R. Dargazany\n",
            "   Veröffentlicht: 30. Juli 2019\n",
            "   Zusammenfassung: Die Vergangenheit, Gegenwart und Zukunft des Deep Learnings wird in dieser Arbeit präsentiert. Angesichts dieser Landschaft und Roadmap prognostizieren wir, dass tiefes kortikales Lernen die Konvergenz von tiefem Lernen und kortikalem Lernen sein wird, das letztendlich eine künstliche kortikale Säule aufbaut.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# question = 'Generate the first 20 numbers in the Fibonacci series.'\n",
        "# question = 'Who is the current prime minister of the UK?'\n",
        "# question = 'Tell me about Napoleon Bonaparte early life'\n",
        "question = 'Find the most recient papers about Deep Learning. The output has to be a list'\n",
        "\n",
        "output = agent_executor.invoke({\n",
        "    'input': prompt_template.format(q=question)\n",
        "})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
